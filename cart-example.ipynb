{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "WARNING: You are using pip version 19.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nWARNING: You are using pip version 19.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nWARNING: You are using pip version 19.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nWARNING: You are using pip version 19.1, however version 19.3.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n"
    }
   ],
   "source": [
    "# %%bash \n",
    "# pip3 -q install gym\n",
    "# pip3 -q install pyglet\n",
    "# pip3 -q install pyopengl\n",
    "# pip3 -q install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "observation space: Box(4,)\naction space: Discrete(2)\ninitial observation: [-0.01728797 -0.03825185 -0.00374553  0.00913405]\nnext observation: [-0.01805301  0.15692362 -0.00356285 -0.28472828]\nreward: 1.0\ndone: False\ninfo: {}\n[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "#env.render()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "\n",
    "\n",
    "from gym import envs\n",
    "# print(envs.registry.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
    }
   ],
   "source": [
    "import gym \n",
    "\n",
    "frames = []\n",
    "# for i in range(3):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     R = 0\n",
    "#     t = 0\n",
    "#     while not done and t < 200:\n",
    "#         frames.append(env.render(mode = 'rgb_array'))\n",
    "#         # define formula for action and put that is step\n",
    "#         #  action = agent.act(obs)\n",
    "#         obs, r, done, _ = env.step(env.action_space.sample())\n",
    "#         R += r\n",
    "#         t += 1\n",
    "#     print('test episode:', i, 'R:', R)\n",
    "#     # agent.stop_episode()\n",
    "# env.render()\n",
    "\n",
    "\n",
    "for _ in range(100):\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()\n",
    "\n",
    "# for inline plotting maybe\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.animation\n",
    "# import numpy as np\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "# patch = plt.imshow(frames[0])\n",
    "# plt.axis('off')\n",
    "# animate = lambda i: patch.set_data(frames[i])\n",
    "# ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
    "# HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Episode finished after 15 timesteps\nEpisode finished after 12 timesteps\nEpisode finished after 25 timesteps\nEpisode finished after 11 timesteps\nEpisode finished after 13 timesteps\nEpisode finished after 18 timesteps\nEpisode finished after 16 timesteps\nEpisode finished after 14 timesteps\nEpisode finished after 21 timesteps\nEpisode finished after 15 timesteps\nEpisode finished after 13 timesteps\nEpisode finished after 32 timesteps\nEpisode finished after 16 timesteps\nEpisode finished after 14 timesteps\nEpisode finished after 39 timesteps\nEpisode finished after 11 timesteps\nEpisode finished after 21 timesteps\nEpisode finished after 27 timesteps\nEpisode finished after 15 timesteps\nEpisode finished after 31 timesteps\n"
    }
   ],
   "source": [
    "import gym\n",
    "frames = [] \n",
    "env = gym.make('CartPole-v0')\n",
    "for episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        frames.append(env.render(mode = 'rgb_array'))\n",
    "        # print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]]\nEpisode 50 Total Reward: 11.0\nEpisode 100 Total Reward: 9.0\nEpisode 150 Total Reward: 8.0\nEpisode 200 Total Reward: 9.0\nEpisode 250 Total Reward: 10.0\nEpisode 300 Total Reward: 10.0\nEpisode 350 Total Reward: 9.0\nEpisode 400 Total Reward: 10.0\nEpisode 450 Total Reward: 9.0\nEpisode 500 Total Reward: 9.0\nEpisode 550 Total Reward: 10.0\nEpisode 600 Total Reward: 9.0\nEpisode 650 Total Reward: 11.0\nEpisode 700 Total Reward: 10.0\nEpisode 750 Total Reward: 10.0\nEpisode 800 Total Reward: 9.0\nEpisode 850 Total Reward: 9.0\nEpisode 900 Total Reward: 9.0\nEpisode 950 Total Reward: 9.0\nEpisode 1000 Total Reward: 9.0\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# apply manual Q-learning https://www.oreilly.com/radar/introduction-to-reinforcement-learning-and-openai-gym/\n",
    "Q = np.zeros([env.observation_space.shape[0], env.action_space.n])\n",
    "print(Q)\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "for episode in range(1,1001):\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset() # creates a random state\n",
    "    i = 0\n",
    "    for s in state:\n",
    "        Q[i,0] = s\n",
    "        i +=1 \n",
    "    # print(Q.shape)    \n",
    "    while done != True:\n",
    "        action = np.argmax(Q[:1]) #1\n",
    "        state2, reward, done, info = env.step(action) #2\n",
    "        i = 0\n",
    "        for s in state:\n",
    "            Q[i,0] += alpha * (reward + np.max(state2[i]) - Q[i,0])\n",
    "            Q[i,1] += alpha * (reward + np.max(state2[i]) - Q[i,1])\n",
    "            i +=1 \n",
    "        # Q[state,action]  #3\n",
    "        G += reward\n",
    "        state = state2 \n",
    "    # print(Q.shape)      \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode,G))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}